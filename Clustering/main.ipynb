{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import timedelta\n",
    "from scipy.fftpack import fft, ifft,rfft\n",
    "from scipy.stats import entropy\n",
    "from scipy.stats import iqr\n",
    "from scipy.signal import periodogram\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from joblib import dump, load\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import cluster\n",
    "import collections\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "insulin_df=pd.read_csv('InsulinData.csv',low_memory=False,usecols=['Date','Time','BWZ Carb Input (grams)'])\n",
    "cgm_df=pd.read_csv('CGMData.csv',low_memory=False,usecols=['Date','Time','Sensor Glucose (mg/dL)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "insulin_df['date_time_stamp']=pd.to_datetime(insulin_df['Date'] + ' ' + insulin_df['Time'])\n",
    "cgm_df['date_time_stamp']=pd.to_datetime(cgm_df['Date'] + ' ' + cgm_df['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "insulin_df = insulin_df.set_index('date_time_stamp')\n",
    "valid_timestamps = []\n",
    "find_carb_inputs = insulin_df.sort_values(by='date_time_stamp', ascending=True).dropna().reset_index()\n",
    "find_carb_inputs['BWZ Carb Input (grams)'].replace(0.0, np.nan, inplace=True)\n",
    "find_carb_inputs = find_carb_inputs.dropna().reset_index().drop(columns='index')\n",
    "ground_truth = []\n",
    "\n",
    "for i, timestamp in enumerate(find_carb_inputs['date_time_stamp']):\n",
    "    try:\n",
    "        time_diff = (find_carb_inputs['date_time_stamp'][i+1] - timestamp).seconds / 60.0\n",
    "        if find_carb_inputs.loc[i, 'BWZ Carb Input (grams)'] > 0 and time_diff >= 120:\n",
    "            valid_timestamps.append(timestamp)\n",
    "            ground_truth.append(find_carb_inputs.loc[find_carb_inputs['date_time_stamp'] == timestamp, 'BWZ Carb Input (grams)'].values[0])\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "meal_data = []\n",
    "for timestamp in valid_timestamps:\n",
    "    start = pd.to_datetime(timestamp - timedelta(minutes=30))\n",
    "    end = pd.to_datetime(timestamp + timedelta(minutes=120))\n",
    "    date_str = timestamp.date().strftime('%#m/%#d/%Y')\n",
    "    meal_data.append(cgm_df.loc[cgm_df['Date'] == date_str].set_index('date_time_stamp').between_time(start_time=start.strftime('%#H:%#M:%#S'),end_time=end.strftime('%#H:%#M:%#S'))['Sensor Glucose (mg/dL)'].values.tolist())\n",
    "meal_data = pd.DataFrame(meal_data)\n",
    "meal_data = meal_data.iloc[:,0:30]\n",
    "meal_data['carbs_ground_truth'] = ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = meal_data.drop(meal_data.isna().sum(axis=1).replace(0, np.nan).dropna().where(lambda x: x > 6).dropna().index).reset_index().drop(columns='index')\n",
    "cleaned_data = cleaned_data.interpolate(method='linear', axis=1)\n",
    "index_to_drop = cleaned_data.isna().sum(axis=1).replace(0, np.nan).dropna().index\n",
    "cleaned_data = cleaned_data.drop(meal_data.index[index_to_drop]).reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_truth = cleaned_data['carbs_ground_truth'].min()\n",
    "max_truth = cleaned_data['carbs_ground_truth'].max()\n",
    "number_of_bins = math.ceil((max_truth - min_truth)/20)\n",
    "bin_values = []\n",
    "for i in cleaned_data['carbs_ground_truth'].tolist():\n",
    "    bin_values.append(int((i-min_truth)/20))\n",
    "ground_truth = cleaned_data['carbs_ground_truth'].tolist()\n",
    "cleaned_data = cleaned_data.drop('carbs_ground_truth', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_differential_max = []\n",
    "first_differential_min = []\n",
    "first_differential_avg = []\n",
    "second_differential_max = []\n",
    "second_differential_min = []\n",
    "second_differential_avg = []\n",
    "entropies = []\n",
    "iqr_by_row = cleaned_data.apply(iqr, axis=1, nan_policy='omit')\n",
    "fft_1 = []\n",
    "fft_2 = []\n",
    "fft_3 = []\n",
    "fft_4 = []\n",
    "fft_5 = []\n",
    "fft_6 = []\n",
    "psd1_mean = []\n",
    "psd2_mean = []\n",
    "psd3_mean = []\n",
    "\n",
    "for i in range(len(cleaned_data)):\n",
    "    fft_results = abs(rfft(cleaned_data.iloc[:, 0:30].iloc[i].values.tolist())).tolist()\n",
    "    fft_results.sort(reverse=True)\n",
    "    fft_1.append(fft_results[0])\n",
    "    fft_2.append(fft_results[1])\n",
    "    fft_3.append(fft_results[2])\n",
    "    fft_4.append(fft_results[3])\n",
    "    fft_5.append(fft_results[4])\n",
    "    fft_6.append(fft_results[5])\n",
    "    \n",
    "for i in range(len(cleaned_data)):\n",
    "    data = cleaned_data.iloc[:,0:30].iloc[i].values.tolist()\n",
    "    #Velocity\n",
    "    first_differential_max.append(np.diff(data).max())\n",
    "    first_differential_min.append(np.diff(data).min())\n",
    "    first_differential_avg.append(np.diff(data).sum()/len(np.diff(data)))\n",
    "    #Acceleration\n",
    "    second_differential_max.append(np.diff(np.diff(data)).max())\n",
    "    second_differential_min.append(np.diff(np.diff(data)).min())\n",
    "    second_differential_avg.append(np.diff(np.diff(data)).sum()/len(np.diff(np.diff(data))))\n",
    "    #Entropy\n",
    "    entropies.append(entropy(data, base=2))\n",
    "    #PSD\n",
    "    frequencies, psd = periodogram((data))\n",
    "    psd1_mean.append(np.mean(psd[0:5]))\n",
    "    psd2_mean.append(np.mean(psd[5:10]))\n",
    "    psd3_mean.append(np.mean(psd[10:16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "meal_features = pd.DataFrame({\n",
    "    'first_differential_max': first_differential_max,\n",
    "    'first_differential_min': first_differential_min,\n",
    "    'first_differential_avg': first_differential_avg,\n",
    "    'second_differential_max': second_differential_max,\n",
    "    'second_differential_min': second_differential_min,\n",
    "    'second_differential_avg': second_differential_avg,\n",
    "    'entropies': entropies,\n",
    "    'iqr_by_row': iqr_by_row,\n",
    "    'fft_1': fft_1,\n",
    "    'fft_2': fft_2,\n",
    "    'fft_3': fft_3,\n",
    "    'fft_4': fft_4,\n",
    "    'fft_5': fft_5,\n",
    "    'fft_6': fft_6,\n",
    "    'psd1_mean': psd1_mean,\n",
    "    'psd2_mean': psd2_mean,\n",
    "    'psd3_mean': psd3_mean\n",
    "})\n",
    "scaler = RobustScaler()\n",
    "meal_features_scaled = scaler.fit_transform(meal_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(y_true, y_pred, base = 2):\n",
    "    contingency_matrix = cluster.contingency_matrix(y_true, y_pred)\n",
    "    base = e if base is None else base\n",
    "    Entropy = []\n",
    "    for i in range(0, len(contingency_matrix)):\n",
    "        p = contingency_matrix[i,:]\n",
    "        p = pd.Series(p).value_counts(normalize=True, sort=False)\n",
    "        Entropy.append((-p/p.sum() * np.log(p/p.sum())/np.log(2)).sum())\n",
    "    TotalP = sum(contingency_matrix,1);\n",
    "    WholeEntropy = 0;\n",
    "    for i in range(0, len(contingency_matrix)):\n",
    "        p = contingency_matrix[i,:]\n",
    "        WholeEntropy = WholeEntropy + ((sum(p))/(sum(TotalP)))*Entropy[i]\n",
    "    return WholeEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_purity_score(y_true, y_pred):\n",
    "    contingency_matrix = cluster.contingency_matrix(y_true, y_pred)\n",
    "    Purity = []\n",
    "    for i in range(0, len(contingency_matrix)):\n",
    "        p = contingency_matrix[i,:]\n",
    "        Purity.append(p.max()/p.sum())\n",
    "    TotalP = sum(contingency_matrix,1);\n",
    "    WholePurity = 0;\n",
    "    for i in range(0, len(contingency_matrix)):\n",
    "        p = contingency_matrix[i,:]\n",
    "        WholePurity = WholePurity + ((sum(p))/(sum(TotalP)))*Purity[i]\n",
    "    return WholePurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Means Bin Cluster matrix:\n",
      " [[92.  7.  1. 20. 31.  4.]\n",
      " [85.  7.  0. 16. 34.  3.]\n",
      " [66.  7.  0. 10. 35.  0.]\n",
      " [29.  3.  0.  7. 18.  1.]\n",
      " [18.  5.  0.  1.  6.  0.]\n",
      " [ 2.  1.  0.  1.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=6, random_state=42)\n",
    "kmeans.fit(meal_features_scaled)\n",
    "KMeans_df = pd.DataFrame({'Ground Truth': bin_values})\n",
    "KMeans_df['KmeanCluster'] = kmeans.labels_\n",
    "kmean_sse = kmeans.inertia_\n",
    "kmean_entropy = calculate_entropy(KMeans_df['Ground Truth'], KMeans_df['KmeanCluster'])\n",
    "kmean_purity = calculate_purity_score(KMeans_df['Ground Truth'], KMeans_df['KmeanCluster'])\n",
    "labels = kmeans.labels_\n",
    "n_clusters = len(set(labels))\n",
    "bin_matrix = np.zeros([6, n_clusters])\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] != -1:\n",
    "        bin_matrix[bin_values[i]][labels[i]]+=1\n",
    "print(\"K Means Bin Cluster matrix:\\n\", bin_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB Scan Bin Cluster matrix:\n",
      " [[44.  5.  2.  0.  2.  0.]\n",
      " [30.  3.  2.  0.  1.  0.]\n",
      " [20.  1.  1.  4.  0.  0.]\n",
      " [12.  1.  0.  0.  0.  0.]\n",
      " [ 7.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "dbscan = DBSCAN(eps=1.115, min_samples=3, metric='euclidean')\n",
    "dbscan_model = dbscan.fit(meal_features_scaled)\n",
    "DBScan = pd.DataFrame({'Ground Truth': bin_values})\n",
    "DBScan['DBScan'] = dbscan_model.labels_\n",
    "cluster_centers = []\n",
    "for label in np.unique(dbscan_model.labels_):\n",
    "    if label != -1:\n",
    "        cluster_centers.append(np.mean(meal_features_scaled[dbscan_model.labels_ == label], axis=0))\n",
    "\n",
    "# Calculate the sum of squared distances for each cluster\n",
    "sse_values = []\n",
    "for i, center in enumerate(cluster_centers):\n",
    "    sse = np.sum(pairwise_distances(meal_features_scaled[dbscan_model.labels_ == i], [center])**2)\n",
    "    sse_values.append(sse)\n",
    "DBScan_sse = np.mean(sse_values)\n",
    "DBScan_entropy = calculate_entropy(DBScan['Ground Truth'], DBScan['DBScan'])\n",
    "DBScan_purity = calculate_purity_score(DBScan['Ground Truth'], DBScan['DBScan'])\n",
    "# Compute bin cluster matrix\n",
    "labels = dbscan_model.labels_\n",
    "n_clusters = len(set(labels))\n",
    "bin_matrix = np.zeros([6, n_clusters])\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] != -1:\n",
    "        bin_matrix[bin_values[i]][labels[i]]+=1\n",
    "print(\"DB Scan Bin Cluster matrix:\\n\", bin_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SSE for Kmeans  SSE for DBSCAN  Entropy for Kmeans  Entropy for DBSCAN  \\\n",
      "0     4449.163556       46.202987            2.448653            2.225344   \n",
      "\n",
      "   Purity for K means  Purity for DBSCAN  \n",
      "0            0.564797           0.725338  \n"
     ]
    }
   ],
   "source": [
    "ldct_result = {}\n",
    "ldct_result['SSE for Kmeans'] =  kmean_sse\n",
    "ldct_result['SSE for DBSCAN'] =  DBScan_sse\n",
    "ldct_result['Entropy for Kmeans'] =  kmean_entropy\n",
    "ldct_result['Entropy for DBSCAN'] =  DBScan_entropy\n",
    "ldct_result['Purity for K means'] =  kmean_purity\n",
    "ldct_result['Purity for DBSCAN'] =  DBScan_purity\n",
    "ldf_result = pd.DataFrame(ldct_result, index=[0])\n",
    "ldf_result.to_csv('Result.csv',index=False,header=False)\n",
    "print(ldf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth Cluster Alloted: [5, 3, 2, 1, 2, 2, 3, 3, 2, 2, 5, 1, 0, 1, 2, 4, 0, 1, 1, 3, 3, 3, 1, 1, 2, 2, 1, 3, 2, 1, 0, 3, 1, 1, 1, 3, 3, 2, 3, 2, 0, 0, 0, 1, 1, 3, 0, 2, 1, 1, 2, 2, 1, 2, 3, 1, 0, 1, 2, 2, 1, 2, 0, 1, 3, 0, 1, 0, 1, 4, 1, 2, 4, 1, 1, 1, 2, 2, 0, 2, 4, 2, 4, 2, 2, 1, 0, 0, 1, 4, 4, 2, 1, 4, 0, 0, 4, 1, 3, 5, 5, 3, 1, 1, 1, 1, 0, 2, 4, 3, 1, 0, 1, 1, 2, 3, 0, 3, 1, 2, 1, 3, 3, 1, 1, 4, 2, 0, 4, 0, 2, 1, 4, 2, 2, 2, 0, 4, 2, 0, 2, 0, 2, 2, 2, 2, 2, 1, 0, 2, 1, 4, 3, 2, 1, 4, 2, 2, 0, 2, 1, 1, 1, 1, 0, 0, 1, 2, 0, 2, 0, 1, 1, 0, 1, 1, 1, 2, 2, 3, 2, 0, 2, 0, 4, 0, 1, 2, 1, 0, 3, 2, 3, 2, 3, 1, 3, 3, 1, 3, 2, 2, 0, 3, 2, 1, 0, 1, 1, 4, 0, 0, 3, 1, 0, 3, 1, 2, 3, 3, 2, 1, 0, 0, 1, 0, 3, 1, 2, 1, 1, 0, 1, 0, 0, 2, 4, 0, 1, 0, 2, 1, 2, 1, 2, 3, 0, 0, 2, 1, 1, 0, 0, 0, 3, 2, 0, 0, 2, 2, 2, 1, 2, 4, 0, 2, 0, 0, 2, 0, 0, 2, 1, 0, 0, 2, 2, 1, 0, 2, 0, 0, 2, 3, 0, 1, 1, 3, 1, 3, 0, 3, 1, 5, 0, 0, 3, 0, 3, 0, 3, 1, 0, 0, 0, 0, 0, 0, 4, 4, 1, 1, 3, 3, 2, 1, 3, 1, 0, 1, 0, 1, 0, 1, 2, 1, 0, 0, 2, 2, 2, 2, 0, 3, 2, 1, 1, 2, 1, 0, 2, 0, 0, 1, 0, 0, 0, 3, 0, 2, 3, 0, 1, 3, 0, 1, 1, 0, 0, 4, 0, 0, 1, 1, 0, 1, 1, 3, 0, 2, 0, 0, 2, 3, 1, 1, 0, 0, 0, 2, 0, 0, 2, 0, 0, 1, 4, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 3, 0, 0, 1, 1, 0, 1, 1, 4, 0, 0, 1, 0, 2, 2, 0, 2, 1, 1, 2, 1, 0, 2, 1, 1, 2, 0, 0, 0, 3, 0, 1, 2, 0, 0, 0, 2, 1, 2, 2, 2, 1, 1, 0, 2, 0, 1, 3, 1, 0, 1, 2, 1, 0, 1, 4, 1, 1, 2, 0, 2, 0, 1, 2, 1, 0, 1, 0, 0, 0, 0, 1, 2, 0, 0, 4, 0, 0, 4, 1, 1, 0, 0, 2, 0, 4, 2, 1, 0, 0, 1, 2, 0, 2, 2, 3, 1, 2, 1, 2, 0, 0, 2, 1, 1, 0, 2, 4, 1, 0, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"Ground Truth Cluster Alloted:\", bin_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth: [155, 145, 118, 58, 30, 5]\n",
      "K Means Cluster Count: [292, 125, 55, 30, 8, 1]\n",
      "DBScan Cluster Count: [375, 113, 10, 5, 5, 3]\n"
     ]
    }
   ],
   "source": [
    "bins = pd.DataFrame(bin_values)\n",
    "print(\"Ground Truth:\", bins[0].value_counts().tolist())\n",
    "print(\"K Means Cluster Count:\", KMeans_df['KmeanCluster'].value_counts().tolist())\n",
    "print(\"DBScan Cluster Count:\", DBScan['DBScan'].value_counts().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
